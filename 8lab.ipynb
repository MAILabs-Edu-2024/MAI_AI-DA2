{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-7ZUN4w1Bv5",
        "outputId": "6a401ce9-2171-41c6-e104-640ca2ebdd30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–†–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ /content/VOC2007/VOCtrainval_06-Nov-2007.tar\n",
            "–†–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ /content/VOC2012/VOCtrainval_11-May-2012.tar\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ tar\n",
        "def extract_tar(path_to_tar, extract_path):\n",
        "    if not os.path.exists(extract_path):\n",
        "        os.makedirs(extract_path)\n",
        "    with tarfile.open(path_to_tar) as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "    print(f\"–†–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ {path_to_tar}\")\n",
        "\n",
        "# –ü—É—Ç–∏ –∫ –∞—Ä—Ö–∏–≤–∞–º\n",
        "tar_2007 = '/content/VOC2007/VOCtrainval_06-Nov-2007.tar'\n",
        "tar_2012 = '/content/VOC2012/VOCtrainval_11-May-2012.tar'\n",
        "\n",
        "# –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º\n",
        "extract_tar(tar_2007, '/content/VOC2007/')\n",
        "extract_tar(tar_2012, '/content/VOC2012/')\n",
        "\n",
        "# –ü—É—Ç–∏ –∫ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
        "VOC2007_DIR = '/content/VOC2007/VOCdevkit/VOC2007'\n",
        "VOC2012_DIR = '/content/VOC2012/VOCdevkit/VOC2012'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UHv5hkdi1L9k"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# –ö–ª–∞—Å—Å—ã VOC (20 –∫–ª–∞—Å—Å–æ–≤)\n",
        "classes = [\n",
        "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
        "    'bus', 'car', 'cat', 'chair', 'cow',\n",
        "    'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "]\n",
        "\n",
        "def list_xml_files(path):\n",
        "    return list(Path(path).glob('*.xml'))\n",
        "\n",
        "def filter_existing_images(annotations, img_dir):\n",
        "    existing_images = set(os.listdir(img_dir))\n",
        "    filtered = []\n",
        "    for ann in annotations:\n",
        "        img_name = ann.stem + '.jpg'\n",
        "        if img_name in existing_images:\n",
        "            filtered.append(ann)\n",
        "        else:\n",
        "            print(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {os.path.join(img_dir, img_name)}\")\n",
        "    return filtered\n",
        "\n",
        "def convert_box(size, box):\n",
        "    dw = 1.0 / size[0]\n",
        "    dh = 1.0 / size[1]\n",
        "    x = (box[0] + box[2]) / 2.0\n",
        "    y = (box[1] + box[3]) / 2.0\n",
        "    w = box[2] - box[0]\n",
        "    h = box[3] - box[1]\n",
        "    return (x * dw, y * dh, w * dw, h * dh)\n",
        "\n",
        "def xml_to_yolo_label(xml_file, classes):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    size = root.find('size')\n",
        "    width = int(float(size.find('width').text))\n",
        "    height = int(float(size.find('height').text))\n",
        "    labels = []\n",
        "    for obj in root.iter('object'):\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text),\n",
        "             float(xmlbox.find('xmax').text), float(xmlbox.find('ymax').text))\n",
        "        bb = convert_box((width, height), b)\n",
        "        labels.append(f\"{cls_id} {' '.join(map(str, bb))}\")\n",
        "    return labels\n",
        "\n",
        "def prepare_dataset_yolo(annotations, img_dir, save_dir):\n",
        "    img_save_train = os.path.join(save_dir, 'images/train')\n",
        "    img_save_val = os.path.join(save_dir, 'images/val')\n",
        "    labels_save_train = os.path.join(save_dir, 'labels/train')\n",
        "    labels_save_val = os.path.join(save_dir, 'labels/val')\n",
        "    os.makedirs(img_save_train, exist_ok=True)\n",
        "    os.makedirs(img_save_val, exist_ok=True)\n",
        "    os.makedirs(labels_save_train, exist_ok=True)\n",
        "    os.makedirs(labels_save_val, exist_ok=True)\n",
        "\n",
        "    random.shuffle(annotations)\n",
        "    split_idx = int(len(annotations) * 0.8)\n",
        "    train_anns = annotations[:split_idx]\n",
        "    val_anns = annotations[split_idx:]\n",
        "\n",
        "    def copy_and_label(anns, img_dest, label_dest):\n",
        "        count = 0\n",
        "        for ann in anns:\n",
        "            img_name = ann.stem + '.jpg'\n",
        "            img_path = os.path.join(img_dir, img_name)\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_path}\")\n",
        "                continue\n",
        "            labels = xml_to_yolo_label(ann, classes)\n",
        "            if not labels:\n",
        "                continue\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º label\n",
        "            label_path = os.path.join(label_dest, ann.stem + '.txt')\n",
        "            with open(label_path, 'w') as f:\n",
        "                f.write('\\n'.join(labels))\n",
        "            # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "            shutil.copy(img_path, img_dest)\n",
        "            count += 1\n",
        "        return count\n",
        "\n",
        "    train_count = copy_and_label(train_anns, img_save_train, labels_save_train)\n",
        "    val_count = copy_and_label(val_anns, img_save_val, labels_save_val)\n",
        "    print(f\"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ train: {train_count}\")\n",
        "    print(f\"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ val: {val_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H9i0JKv1On8",
        "outputId": "9219e44a-b480-4348-9c8a-0e262fed05ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOC2007: 5011 –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
            "VOC2012: 17125 –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º VOC2007 –∏ VOC2012\n",
        "VOC2007_DIR = '/content/VOC2007/VOCdevkit/VOC2007'\n",
        "VOC2012_DIR = '/content/VOC2012/VOCdevkit/VOC2012'\n",
        "\n",
        "def list_xml_files(annotations_dir):\n",
        "    return [os.path.join(annotations_dir, f) for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
        "\n",
        "def filter_existing_images(annotations, img_dir):\n",
        "    existing_images = set(os.listdir(img_dir))\n",
        "    filtered = []\n",
        "    for ann in annotations:\n",
        "        img_filename = os.path.splitext(os.path.basename(ann))[0] + '.jpg'\n",
        "        if img_filename in existing_images:\n",
        "            filtered.append(ann)\n",
        "        else:\n",
        "            print(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {os.path.join(img_dir, img_filename)}\")\n",
        "    return filtered\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\n",
        "voc2007_ann = list_xml_files(os.path.join(VOC2007_DIR, 'Annotations'))\n",
        "voc2012_ann = list_xml_files(os.path.join(VOC2012_DIR, 'Annotations'))\n",
        "\n",
        "# –§–∏–ª—å—Ç—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏, —É–¥–∞–ª—è—è —Ç–µ, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "voc2007_ann = filter_existing_images(voc2007_ann, os.path.join(VOC2007_DIR, 'JPEGImages'))\n",
        "voc2012_ann = filter_existing_images(voc2012_ann, os.path.join(VOC2012_DIR, 'JPEGImages'))\n",
        "\n",
        "print(f\"VOC2007: {len(voc2007_ann)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\")\n",
        "print(f\"VOC2012: {len(voc2012_ann)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\")\n",
        "\n",
        "# –î–∞–ª—å—à–µ –≤—ã–∑—ã–≤–∞–π —Å–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
        "# prepare_dataset_yolo(voc2007_ann + voc2012_ann)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrmPAdKf1Pv8",
        "outputId": "636cbd39-de59-45ed-ce58-6155dad56843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.134 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=voc_data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1221.4¬±796.1 MB/s, size: 83.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/voc_yolo/labels/train... 3973 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3973/3973 [00:01<00:00, 2180.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/voc_yolo/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 619.1¬±166.2 MB/s, size: 83.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/voc_yolo/labels/val... 1038 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1038/1038 [00:00<00:00, 1781.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/voc_yolo/labels/val.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 320 train, 320 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/5     0.354G      1.296      3.231      1.281         44        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:55<00:00,  8.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:09<00:00,  7.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.601      0.399      0.449      0.303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/5     0.408G      1.328      2.161       1.31         21        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:51<00:00,  9.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:06<00:00,  9.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.604      0.496      0.519      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/5     0.426G      1.287      1.955      1.298         20        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:51<00:00,  9.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:06<00:00, 10.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.606      0.518      0.534      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        4/5     0.434G      1.248      1.821      1.274         24        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:50<00:00,  9.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:07<00:00,  8.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.628      0.537      0.576      0.395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        5/5     0.461G      1.215      1.705      1.255         26        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:50<00:00,  9.91it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:06<00:00, 10.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.678      0.561      0.616      0.419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 epochs completed in 0.082 hours.\n",
            "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train4/weights/best.pt...\n",
            "Ultralytics 8.3.134 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:07<00:00,  8.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.679      0.561      0.616       0.42\n",
            "             aeroplane         51         87      0.701      0.747      0.765      0.525\n",
            "               bicycle         60         95      0.785      0.616      0.761      0.483\n",
            "                  bird         67        153      0.713      0.431      0.528      0.321\n",
            "                  boat         43         87      0.422      0.494      0.461      0.267\n",
            "                bottle         49        104       0.58      0.305      0.363      0.204\n",
            "                   bus         37         57      0.733      0.596      0.636      0.513\n",
            "                   car        154        293      0.794      0.706      0.774      0.588\n",
            "                   cat         65         73      0.752      0.781      0.833      0.605\n",
            "                 chair        119        312      0.556      0.308      0.369      0.216\n",
            "                   cow         36        106      0.608      0.472      0.558      0.353\n",
            "           diningtable         54         74      0.655      0.527      0.589      0.409\n",
            "                   dog         99        127      0.702      0.501      0.638      0.488\n",
            "                 horse         65         90      0.766      0.733      0.784      0.555\n",
            "             motorbike         47         74      0.763       0.61      0.683      0.478\n",
            "                person        424       1153      0.858      0.637      0.762       0.48\n",
            "           pottedplant         66        159      0.561      0.346      0.369      0.182\n",
            "                 sheep         25         95      0.532      0.505      0.474      0.258\n",
            "                  sofa         67         79        0.6      0.595      0.609      0.476\n",
            "                 train         60         78      0.819      0.695      0.751      0.545\n",
            "             tvmonitor         45         57      0.676      0.614       0.62      0.455\n",
            "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
            "–û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ YOLOv8n –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\n"
          ]
        }
      ],
      "source": [
        "# –Ø—á–µ–π–∫–∞ 4: –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ YOLOv8n (5 —ç–ø–æ—Ö)\n",
        "\n",
        "model_n = YOLO('yolov8n.pt')\n",
        "\n",
        "results_n = model_n.train(data='voc_data.yaml', epochs=5, imgsz=320, batch=8)\n",
        "\n",
        "print(\"–û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ YOLOv8n –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyPDzDPA1RFS",
        "outputId": "264c0c65-4354-464d-ca99-0cbde59f2fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.5M/21.5M [00:00<00:00, 228MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.134 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=voc_data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2123788  ultralytics.nn.modules.head.Detect           [20, [128, 256, 512]]         \n",
            "Model summary: 129 layers, 11,143,340 parameters, 11,143,324 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1968.0¬±431.3 MB/s, size: 83.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/voc_yolo/labels/train.cache... 3973 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3973/3973 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 535.1¬±95.0 MB/s, size: 83.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/voc_yolo/labels/val.cache... 1038 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1038/1038 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 320 train, 320 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/10     0.787G      1.091      1.935      1.147          7        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:54<00:00,  9.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:07<00:00,  8.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.634      0.597      0.619      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/10      1.03G      1.154      1.499       1.19         12        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:50<00:00,  9.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:06<00:00, 10.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.602      0.537      0.564      0.361\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/10      1.03G      1.196      1.503       1.23         30        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:51<00:00,  9.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:07<00:00,  9.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.587      0.519      0.553      0.354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/10      1.05G      1.196      1.427      1.236         21        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:51<00:00,  9.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:07<00:00,  9.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353       0.66      0.538      0.584      0.385\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 3 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=3) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from runs/detect/train5/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train5/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train5/weights/best.pt...\n",
            "Ultralytics 8.3.134 üöÄ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,133,324 parameters, 0 gradients, 28.5 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:10<00:00,  6.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1038       3353      0.602      0.593      0.611      0.421\n",
            "             aeroplane         51         87      0.577      0.564      0.626       0.45\n",
            "               bicycle         60         95      0.887      0.526       0.69      0.449\n",
            "                  bird         67        153      0.412      0.477       0.44      0.303\n",
            "                  boat         43         87      0.256      0.657      0.441      0.228\n",
            "                bottle         49        104      0.473      0.615      0.515      0.316\n",
            "                   bus         37         57      0.628      0.653      0.662      0.532\n",
            "                   car        154        293      0.733      0.761      0.805      0.617\n",
            "                   cat         65         73      0.788      0.612      0.768      0.575\n",
            "                 chair        119        312      0.513      0.506      0.478      0.284\n",
            "                   cow         36        106       0.88      0.104      0.496      0.315\n",
            "           diningtable         54         74      0.795      0.523      0.592       0.42\n",
            "                   dog         99        127      0.495      0.795      0.694      0.556\n",
            "                 horse         65         90      0.744      0.548      0.667      0.481\n",
            "             motorbike         47         74      0.681      0.636      0.703      0.454\n",
            "                person        424       1153      0.749      0.717      0.787      0.521\n",
            "           pottedplant         66        159      0.669      0.314      0.434      0.243\n",
            "                 sheep         25         95      0.282      0.674      0.448      0.275\n",
            "                  sofa         67         79      0.508      0.646      0.588      0.451\n",
            "                 train         60         78      0.517       0.85       0.77      0.522\n",
            "             tvmonitor         45         57      0.463      0.684      0.625      0.435\n",
            "Speed: 0.1ms preprocess, 4.7ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n",
            "–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv8s –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\n"
          ]
        }
      ],
      "source": [
        "# –Ø—á–µ–π–∫–∞ 5: –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv8s (10 —ç–ø–æ—Ö, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)\n",
        "\n",
        "model_s = YOLO('yolov8s.pt')\n",
        "\n",
        "results_s = model_s.train(\n",
        "    data='voc_data.yaml',\n",
        "    epochs=10,\n",
        "    imgsz=320,\n",
        "    batch=8,\n",
        "    augment=True,\n",
        "    lr0=0.001,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "print(\"–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv8s –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3SXkHC1VpV",
        "outputId": "c8bcac53-602b-4001-9d20-4b3a646f97a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Model  Precision   Recall   mAP50  mAP50-95\n",
            "0  YOLOv8n    0.67750  0.56131  0.6158   0.41901\n",
            "1  YOLOv8s    0.65967  0.53773  0.5844   0.38486\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_models_metrics_from_csv(run_dirs, model_names):\n",
        "    data = []\n",
        "    for run_dir, model_name in zip(run_dirs, model_names):\n",
        "        results_path = f\"{run_dir}/results.csv\"\n",
        "        df = pd.read_csv(results_path)\n",
        "\n",
        "        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É (–ø–æ—Å–ª–µ–¥–Ω—é—é —ç–ø–æ—Ö—É)\n",
        "        last_epoch = df.iloc[-1]\n",
        "\n",
        "        precision = last_epoch['metrics/precision(B)']\n",
        "        recall = last_epoch['metrics/recall(B)']\n",
        "        mAP50 = last_epoch['metrics/mAP50(B)']\n",
        "        mAP50_95 = last_epoch['metrics/mAP50-95(B)']\n",
        "\n",
        "        data.append({\n",
        "            'Model': model_name,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'mAP50': mAP50,\n",
        "            'mAP50-95': mAP50_95\n",
        "        })\n",
        "\n",
        "    compare_df = pd.DataFrame(data)\n",
        "    print(compare_df)\n",
        "\n",
        "# –ü—É—Ç–∏ –∫ –ø–∞–ø–∫–∞–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "run_dirs = ['/content/runs/detect/train4', '/content/runs/detect/train5']\n",
        "model_names = ['YOLOv8n', 'YOLOv8s']\n",
        "\n",
        "compare_models_metrics_from_csv(run_dirs, model_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSxmFezqP_UU",
        "outputId": "d1d0999d-4a73-4a3f-fa81-b6f633445dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–î–ª–∏–Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: 3973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987/1987 [loss=0.652]\n",
            "Epoch 2/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987/1987 [loss=0.626]\n",
            "Epoch 3/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987/1987 [loss=0.619]\n",
            "Epoch 4/4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987/1987 [loss=0.614]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –ø–∞—Ä —Ñ–∞–π–ª–æ–≤ (image, mask)\n",
        "def get_matched_file_pairs(images_dir, masks_dir):\n",
        "    images = os.listdir(images_dir)\n",
        "    masks = os.listdir(masks_dir)\n",
        "\n",
        "    images_set = set(os.path.splitext(f)[0] for f in images)\n",
        "    masks_set = set(os.path.splitext(f)[0] for f in masks)\n",
        "\n",
        "    matched_names = images_set.intersection(masks_set)\n",
        "\n",
        "    matched_images = [os.path.join(images_dir, f\"{name}.jpg\") for name in matched_names]\n",
        "    matched_masks = [os.path.join(masks_dir, f\"{name}.png\") for name in matched_names]\n",
        "\n",
        "    return matched_images, matched_masks\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images, self.masks = get_matched_file_pairs(images_dir, masks_dir)\n",
        "        assert len(self.images) == len(self.masks), \"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.images[idx]).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(self.masks[idx]).convert(\"L\"))\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # –ú–∞—Å–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å FloatTensor —Å –æ–¥–Ω–∏–º –∫–∞–Ω–∞–ª–æ–º\n",
        "        if len(mask.shape) == 2:\n",
        "            mask = np.expand_dims(mask, axis=0)\n",
        "        mask = torch.tensor(mask).float() / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ [0,1]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ä–∞–∑–º–µ—Ä–æ–º 128x128\n",
        "transform = A.Compose([\n",
        "    A.Resize(128, 128),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# –ü—É—Ç–∏ –∫ –ø–∞–ø–∫–∞–º\n",
        "images_dir = '/content/voc_yolo/images/train'\n",
        "masks_dir = '/content/voc_yolo/masks/train'\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫ —Å batch_size=2\n",
        "train_dataset = SegmentationDataset(images_dir, masks_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"–î–ª–∏–Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(train_dataset)}\")\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
        "class SimpleSegmentationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        x = self.deconv1(x)\n",
        "        x = self.deconv2(x)\n",
        "        x = self.deconv3(x)\n",
        "\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "\n",
        "# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "model = SimpleSegmentationModel().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Å tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–±–∞—Ä–∞\n",
        "epochs = 4  # –º–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –º–∞—Å–æ–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "        if outputs.shape != masks.shape:\n",
        "            outputs = nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_postfix(loss=running_loss / (batch_idx + 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqmw7hEGW8Jx",
        "outputId": "a74a35ff-8d47-4726-88bd-e65214750117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     Model  Precision    Recall     mAP50  mAP50-95\n",
            "0  SimpleSegmentationModel   0.736508  0.687846  0.663216  0.454251\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_score, recall_score, jaccard_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_segmentation_model(model, dataloader, device, max_batches=20, threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    count_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            if outputs.shape != masks.shape:\n",
        "                outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            preds = (outputs > threshold).float()\n",
        "\n",
        "            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤ numpy –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫\n",
        "            all_preds.append(preds.cpu().numpy().reshape(-1))\n",
        "            all_targets.append(masks.cpu().numpy().reshape(-1))\n",
        "\n",
        "            count_batches += 1\n",
        "            if count_batches >= max_batches:\n",
        "                break\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
        "    iou = jaccard_score(all_targets, all_preds, average='binary')\n",
        "\n",
        "\n",
        "    return precision, recall, mAP50, mAP50_95, iou\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏\n",
        "precision, recall, mAP50, mAP50_95, iou = evaluate_segmentation_model(model, train_loader, device)\n",
        "\n",
        "# –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['SimpleSegmentationModel'],\n",
        "    'Precision': [precision],\n",
        "    'Recall': [recall],\n",
        "    'mAP50': [mAP50],\n",
        "    'mAP50-95': [mAP50_95],\n",
        "})\n",
        "\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã —á—Ç–æ —Ä–µ–∫–æ–ª —É —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—ã—à–µ, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –ª—É—á—à–µ –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—ä–µ–∫—Ç—ã, –æ–¥–Ω–∞–∫–æ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ –ø—Ä–µ—Å–∏–∂–µ–Ω –º–µ–Ω—å—à–µ, —á–µ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –¥–µ–ª–∞–µ—Ç –±–æ–ª—å—à–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
